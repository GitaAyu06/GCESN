{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import scipy\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.io import loadmat\n",
    "\n",
    "from scipy.sparse.linalg import eigsh, ArpackError\n",
    "from scipy.sparse import csgraph\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch_geometric.utils as utils\n",
    "from torch_geometric.utils import to_networkx\n",
    "from torch_geometric.nn import GCNConv, global_mean_pool\n",
    "from torch_geometric.datasets import TUDataset, Planetoid\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.data import Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adopted from https://github.com/basiralab/ReMI-Net-Star/tree/9f427d47e99dfef495119b82f43ceb06e3272bbe\n",
    "# Antivectorize given vector into adjacency matrix\n",
    "def antiVectorize(vec, m):\n",
    "    M = np.zeros((m,m))\n",
    "    M[np.tril_indices(m,k=-1)] = vec\n",
    "    M= M.transpose()\n",
    "    M[np.tril_indices(m,k=-1)] = vec\n",
    "    return M\n",
    "\n",
    "# Vectorize graph adjacency matrix into a vector\n",
    "def vectorize(M):\n",
    "    return M[np.tril_indices(M.shape[0], k=-1)]\n",
    "\n",
    "def multivariate_simulate(n_samples=200,n_time=2,n_views=4):\n",
    "    # Note that changing the node count is not provided right now, since we use correlation matrix\n",
    "    # and the mean values of connectivities from real data and it is for 35 nodes.\n",
    "    \n",
    "    # Import all required statistical information.\n",
    "    allstats = np.load(\"./stats/REALDATA_LH_AVGMEANS.npy\") # Connectivity mean values of LH. You can also try with RH.\n",
    "    allcorrs = np.load(\"./stats/REALDATA_LH_AVGCORRS.npy\") # Correlation matrix in LH. You can also try with RH.\n",
    "    all_diffs = np.load(\"./stats/REAL_TIME_DIFF.npy\") # This is an overall representation of time differences in both (LH and RH) datasets.\n",
    "    \n",
    "    times = []\n",
    "    for t in range(n_time):\n",
    "        views = []\n",
    "        for v in range(n_views):\n",
    "            # Note that we randomly assign a new random state to ensure it will generate a different dataset at each run.\n",
    "            # Generate data with the correlations and mean values at the current timepoint.\n",
    "            if t < 2:\n",
    "                connectomic_means = allstats[t,v]\n",
    "                data = multivariate_normal.rvs(connectomic_means,allcorrs[t,v],n_samples,random_state=randint(1,9999))\n",
    "            # If the requested timepoints are more than we have in real data, use the correlation information from the last timepoint.\n",
    "            else:\n",
    "                connectomic_means = allstats[-1,v]\n",
    "                data = multivariate_normal.rvs(connectomic_means,allcorrs[-1,v],n_samples,random_state=randint(1,9999))\n",
    "\n",
    "            adj = []\n",
    "            for idx, sample in enumerate(data):\n",
    "                # Create adjacency matrix.\n",
    "                matrix = antiVectorize(sample,35)\n",
    "                # Perturb the real time difference with nonlinear tanh function.\n",
    "                noise = np.tanh( t / n_time )\n",
    "                # Further timepoints will have more significant difference from the baseline (t=6 >> t=1).\n",
    "                matrix = matrix + all_diffs[:,:,v] * ( noise + 1 )\n",
    "                adj.append(matrix)\n",
    "            views.append(np.array(adj))\n",
    "\n",
    "        times.append(np.array(views))\n",
    "    \n",
    "    alldata=np.array(times)\n",
    "    alldata = np.transpose(alldata,(2,0,3,4,1))\n",
    "    return alldata \n",
    "\n",
    "def prepare_data(new_data=False, n_samples=200, n_times=6):\n",
    "    # Note that data with 200 samples and 6 timepoints is very large (5.8M data points),\n",
    "    # check your GPU memory to make sure there is enough space to allocate. If not, try:\n",
    "    # - to reduce dataset size by changing n_samples or n_times.\n",
    "    # - on CPU (this will allocate memory on RAM) --> This might work for example if you have 1GB GPU memory but 16GB RAM.\n",
    "    # - on another computer with a better NVIDIA graphics card. --> 2GB GPU memory will not be enough for 5.8M data.\n",
    "    try:\n",
    "        if new_data:\n",
    "            samples = multivariate_simulate(n_samples,n_times)\n",
    "            np.save('./multivariate_simulation_data.npy',samples)\n",
    "        else:\n",
    "            samples = np.load('./multivariate_simulation_data.npy')\n",
    "    except:\n",
    "        samples = multivariate_simulate(n_samples,n_times)\n",
    "        np.save('./multivariate_simulation_data.npy',samples)\n",
    "    return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = prepare_data(new_data=True,n_samples=100,n_times=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EMCI-AD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "data_path = '/Users/gitaayusalsabila/Documents/0thesis/datasets/eMCI_AD_data/'\n",
    "data_t1 = scipy.io.loadmat(data_path + '67subjectslh_view2_t1.mat')['view2']\n",
    "data_t2 = scipy.io.loadmat(data_path + '67subjectslh_view2_t2.mat')['view2']\n",
    "labels = scipy.io.loadmat(data_path + 'label_dataLH.mat')['label_data']\n",
    "print(labels.shape)\n",
    "\n",
    "# Define the antiVectorize function\n",
    "def antiVectorize(vec, m):\n",
    "    M = np.zeros((m, m))\n",
    "    M[np.tril_indices(m, k=-1)] = vec\n",
    "    M = M.transpose()\n",
    "    M[np.tril_indices(m, k=-1)] = vec\n",
    "    return M\n",
    "\n",
    "# Number of subjects and size of the adjacency matrix\n",
    "num_subjects = data_t1.shape[0]\n",
    "matrix_size = int((1 + np.sqrt(1 + 8 * data_t1.shape[1])) / 2)\n",
    "\n",
    "# Initialize arrays for the results\n",
    "emci_ad_timepoints = np.zeros((num_subjects, 2, matrix_size, matrix_size))\n",
    "emci_ad_timepoints_labeled = np.zeros((num_subjects, 2, 1, matrix_size, matrix_size))\n",
    "\n",
    "# Process each subject's data\n",
    "for i in range(num_subjects):\n",
    "    emci_ad_timepoints[i, 0] = antiVectorize(data_t1[i], matrix_size)\n",
    "    emci_ad_timepoints[i, 1] = antiVectorize(data_t2[i], matrix_size)\n",
    "\n",
    "# Save the arrays to .npy files\n",
    "np.save('emci-ad_timepoints.npy', emci_ad_timepoints)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SLIM160"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_data_from_mat(file_path):\n",
    "    mat_data = loadmat(file_path)\n",
    "    return mat_data['r']\n",
    "\n",
    "def traverse_and_extract_complete_sessions(base_path,atlas_name):\n",
    "    subject_data = []\n",
    "    subjects = os.listdir(base_path)\n",
    "    \n",
    "    for subject in subjects:\n",
    "        subject_path = os.path.join(base_path, subject)\n",
    "        if os.path.isdir(subject_path):\n",
    "            sessions = ['session_1', 'session_2', 'session_3']\n",
    "            session_data = []\n",
    "            for session in sessions:\n",
    "                session_path = os.path.join(subject_path, session)\n",
    "                if os.path.isdir(session_path):\n",
    "                    atlas_path = os.path.join(session_path, atlas_name)\n",
    "                    mat_file_path = os.path.join(atlas_path, 'ROI_FC.mat')\n",
    "                    if os.path.isfile(mat_file_path):\n",
    "                        print(f\"Extracting data from: {mat_file_path}\")\n",
    "                        data = extract_data_from_mat(mat_file_path)\n",
    "                        session_data.append(data)\n",
    "            # Check if all 3 sessions are present\n",
    "            if len(session_data) == 3:\n",
    "                subject_data.append(session_data)\n",
    "    \n",
    "    return np.array(subject_data)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base directory containing subject folders\n",
    "path_160 = '/Users/gitaayusalsabila/Documents/0 Thesis/datasets/slim/SWU'  # Replace this with the actual path\n",
    "atlas_name_160 = 'Dosenbach_160'\n",
    "# Extract the data\n",
    "slim160 = traverse_and_extract_complete_sessions(path_160,atlas_name_160)\n",
    "\n",
    "# Save the consolidated data to a numpy file for later use\n",
    "np.save('slim160.npy', slim160)\n",
    "\n",
    "# Print the shape of the consolidated data\n",
    "print(f\"Consolidated data shape: {slim160.shape}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "doc_task_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
