{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install torch-geometric\n",
    "%pip install pynvml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cpu\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import time\n",
    "import copy\n",
    "import psutil\n",
    "import pynvml\n",
    "import random\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Subset, SubsetRandomSampler, random_split\n",
    "\n",
    "import torch_geometric.utils as utils\n",
    "from torch_geometric.utils import to_networkx\n",
    "from torch_geometric.nn import GCNConv, global_mean_pool\n",
    "from torch_geometric.datasets import TUDataset, Planetoid\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from scipy.sparse import csgraph\n",
    "from scipy.sparse.linalg import eigsh\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from tqdm import tqdm\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    torch.cuda.manual_seed(42)\n",
    "    torch.cuda.manual_seed_all(42)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(f'using {device}')\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)  \n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "def seed_worker(worker_id):\n",
    "    worker_seed = torch.initial_seed() % 2**32\n",
    "    np.random.seed(worker_seed)\n",
    "    random.seed(worker_seed)\n",
    "\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = '/notebooks/dataset/'\n",
    "\n",
    "\n",
    "def data_cleansing(dataset):\n",
    "    # Replace negative values with 0\n",
    "    dataset[dataset < 0] = 0\n",
    "    \n",
    "    # Replace NaN values with 0\n",
    "    dataset = np.nan_to_num(dataset, nan=0)\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "def check_and_drop_invalid_graphs(graph_dataset):\n",
    "    num_graphs, num_timepoints, num_nodes, _ = graph_dataset.shape\n",
    "    num_dimensions = 1\n",
    "    \n",
    "    valid_graphs = []\n",
    "\n",
    "    for i in range(num_graphs):\n",
    "        is_valid = True\n",
    "        for t in range(num_timepoints):\n",
    "            adj_matrix = graph_dataset[i, t, :, :]\n",
    "            num_edges = np.sum(adj_matrix > 0)\n",
    "            if num_edges == 0:\n",
    "                is_valid = False\n",
    "                break\n",
    "        \n",
    "        if is_valid:\n",
    "            valid_graphs.append(i)\n",
    "    \n",
    "    cleaned_dataset = graph_dataset[valid_graphs, :, :, :]\n",
    "    \n",
    "    return cleaned_dataset\n",
    "\n",
    "def convert_to_pyg_data(adj_matrices, inp_features):\n",
    "    pyg_data_list = []\n",
    "    \n",
    "    num_graphs = adj_matrices.shape[0]\n",
    "    num_timepoints = adj_matrices.shape[1]\n",
    "    \n",
    "    for i in range(num_graphs):\n",
    "        for t in range(num_timepoints):\n",
    "            adj_matrix = adj_matrices[i, t]\n",
    "            features = inp_features[i, t]\n",
    "            \n",
    "            # Get edge indices\n",
    "            edge_index = torch.tensor(np.array(adj_matrix.nonzero()), dtype=torch.long)\n",
    "            \n",
    "            # Get edge attributes\n",
    "            edge_attr = torch.tensor(adj_matrix[adj_matrix.nonzero()], dtype=torch.float)\n",
    "            \n",
    "            # Node features\n",
    "            x = torch.tensor(features, dtype=torch.float)\n",
    "            \n",
    "            # Label (timepoint)\n",
    "            y = torch.tensor([t], dtype=torch.long)\n",
    "            \n",
    "            pyg_data = Data(x=x, edge_index=edge_index, edge_attr=edge_attr, y=y)\n",
    "            pyg_data_list.append(pyg_data)\n",
    "    \n",
    "    return pyg_data_list\n",
    "\n",
    "def get_dataset_info(pyg_data_list):\n",
    "    num_features = pyg_data_list[0].x.shape[1]\n",
    "    all_labels = torch.cat([data.y for data in pyg_data_list])\n",
    "    num_classes = torch.unique(all_labels).numel()\n",
    "    return num_features, num_classes\n",
    "\n",
    "def create_data_loaders(pyg_data_list, batch_size, seed=42):\n",
    "    set_seed(seed)\n",
    "    train_val_data, test_data = train_test_split(pyg_data_list, test_size=0.2, random_state=seed)\n",
    "    train_data, val_data = train_test_split(train_val_data, test_size=0.1, random_state=seed)\n",
    "    train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
    "    return train_loader, val_loader, test_loader\n",
    "\n",
    "def process_connectomic_dataset(dataset_name, adj, features, batch_size=32):\n",
    "    print(f'\\n{dataset_name} Dataset')\n",
    "    pyg_data_list = convert_to_pyg_data(adj, features)\n",
    "    print(pyg_data_list[0])\n",
    "    \n",
    "    num_features, num_classes = get_dataset_info(pyg_data_list)\n",
    "    print(f'Num Features = {num_features}, Num Classes = {num_classes}')\n",
    "    \n",
    "    return pyg_data_list, num_features, num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "simulated = np.load(dataset_path + 'simulated_adj.npy')\n",
    "simulated_cleaned = data_cleansing(simulated[:,:,:,:,0])\n",
    "simulated_adj = check_and_drop_invalid_graphs(simulated_cleaned)\n",
    "\n",
    "## EMCI-AD Dataset\n",
    "emci = np.load(dataset_path + 'emci-ad_adj.npy')\n",
    "emci_cleaned = data_cleansing(emci)\n",
    "emci_adj = check_and_drop_invalid_graphs(emci_cleaned)\n",
    "\n",
    "## SLIM160 Dataset\n",
    "slim160 = np.load(dataset_path + 'slim160_adj.npy')\n",
    "slim160_cleaned = data_cleansing(slim160)\n",
    "slim160_adj = check_and_drop_invalid_graphs(slim160_cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Node Features Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_with_ones(graphs, feature_dim=8):\n",
    "    \"\"\"\n",
    "    Initialize node features with ones.\n",
    "    \n",
    "    :param graphs: numpy array of shape [n_subjects, n_timepoints, n_nodes, n_nodes]\n",
    "    :param feature_dim: Dimension of the node features\n",
    "    :return: numpy array of shape [n_subjects, n_timepoints, n_nodes, feature_dim]\n",
    "    \"\"\"\n",
    "    n_subjects, n_timepoints, n_nodes, _ = graphs.shape\n",
    "    node_features = np.ones((n_subjects, n_timepoints, n_nodes, feature_dim))\n",
    "    \n",
    "    return node_features\n",
    "\n",
    "def initialize_with_random(graphs, feature_dim=8):\n",
    "    \"\"\"\n",
    "    Initialize node features with random values.\n",
    "    \n",
    "    :param graphs: numpy array of shape [n_subjects, n_timepoints, n_nodes, n_nodes]\n",
    "    :param feature_dim: Dimension of the node features\n",
    "    :return: numpy array of shape [n_subjects, n_timepoints, n_nodes, feature_dim]\n",
    "    \"\"\"\n",
    "    n_subjects, n_timepoints, n_nodes, _ = graphs.shape\n",
    "    node_features = np.random.rand(n_subjects, n_timepoints, n_nodes, feature_dim)\n",
    "    \n",
    "    return node_features\n",
    "\n",
    "def laplacian_positional_encoding(adj_matrix, dim, ncv=None):\n",
    "    laplacian = csgraph.laplacian(adj_matrix, normed=True)\n",
    "    try:\n",
    "        eigvals, eigvecs = eigsh(laplacian, k=dim + 1, which='SM', ncv=ncv)\n",
    "        encoding = torch.tensor(eigvecs[:, 1:], dtype=torch.float)  # Skip the first eigenvector\n",
    "    except Exception as e:\n",
    "        print(f\"Error for matrix with shape {adj_matrix.shape}: {e}\")\n",
    "        encoding = torch.zeros((adj_matrix.shape[0], dim), dtype=torch.float)  # Fallback to zero encoding\n",
    "    return encoding\n",
    "\n",
    "def process_graph(args):\n",
    "    i, t, graphs, num_nodes, feature_dim = args\n",
    "    adj_matrix = graphs[i, t, :, :]\n",
    "    return (i, t, laplacian_positional_encoding(adj_matrix, feature_dim).numpy())\n",
    "\n",
    "def laplacian_initialization(graphs, feature_dim=8):\n",
    "    \"\"\"\n",
    "    Initialize node features using Laplacian positional encoding.\n",
    "    \n",
    "    :param graphs: numpy array of shape [n_subjects, n_timepoints, n_nodes, n_nodes]\n",
    "    :param feature_dim: Dimension of the node features\n",
    "    :return: numpy array of shape [n_subjects, n_timepoints, n_nodes, feature_dim]\n",
    "    \"\"\"\n",
    "    n_subjects, n_timepoints, n_nodes, _ = graphs.shape\n",
    "    node_features = np.zeros((n_subjects, n_timepoints, n_nodes, feature_dim))\n",
    "    \n",
    "    tasks = [(i, t, graphs, n_nodes, feature_dim) for i in range(n_subjects) for t in range(n_timepoints)]\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        for i, t, encoding in tqdm(executor.map(process_graph, tasks), total=len(tasks)):\n",
    "            node_features[i, t, :, :] = encoding\n",
    "\n",
    "    return node_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [00:00<00:00, 604.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SIMULATED -> ones shape: (100, 3, 35, 8), random shape:(100, 3, 35, 8), laplacian shape:(100, 3, 35, 8)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 134/134 [00:00<00:00, 136.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EMCI-AD -> ones shape: (67, 2, 35, 8), random shape:(67, 2, 35, 8), laplacian shape:(67, 2, 35, 8)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 327/327 [00:00<00:00, 344.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SLIM160 -> ones shape: (109, 3, 160, 8), random shape:(109, 3, 160, 8), laplacian shape:(109, 3, 160, 8)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "simulated_ones_features = initialize_with_ones(simulated_adj)\n",
    "simulated_random_features = initialize_with_random(simulated_adj)\n",
    "simulated_laplacian_features = laplacian_initialization(simulated_adj)\n",
    "print(f'SIMULATED -> ones shape: {simulated_ones_features.shape}, random shape:{simulated_random_features.shape}, laplacian shape:{simulated_laplacian_features.shape}')\n",
    "\n",
    "emci_ones_features = initialize_with_ones(emci_adj)\n",
    "emci_random_features = initialize_with_random(emci_adj)\n",
    "emci_laplacian_features = laplacian_initialization(emci_adj)\n",
    "print(f'EMCI-AD -> ones shape: {emci_ones_features.shape}, random shape:{emci_random_features.shape}, laplacian shape:{emci_laplacian_features.shape}')\n",
    "\n",
    "slim160_ones_features = initialize_with_ones(slim160_adj)\n",
    "slim160_random_features = initialize_with_random(slim160_adj)\n",
    "slim160_laplacian_features = laplacian_initialization(slim160_adj)\n",
    "print(f'SLIM160 -> ones shape: {slim160_ones_features.shape}, random shape:{slim160_random_features.shape}, laplacian shape:{slim160_laplacian_features.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Simulated-ones Dataset\n",
      "Data(x=[35, 8], edge_index=[2, 1168], edge_attr=[1168], y=[1])\n",
      "Num Features = 8, Num Classes = 3\n",
      "\n",
      "Simulated-random Dataset\n",
      "Data(x=[35, 8], edge_index=[2, 1168], edge_attr=[1168], y=[1])\n",
      "Num Features = 8, Num Classes = 3\n",
      "\n",
      "Simulated-laplacian Dataset\n",
      "Data(x=[35, 8], edge_index=[2, 1168], edge_attr=[1168], y=[1])\n",
      "Num Features = 8, Num Classes = 3\n",
      "\n",
      "EMCI-AD-ones Dataset\n",
      "Data(x=[35, 8], edge_index=[2, 1178], edge_attr=[1178], y=[1])\n",
      "Num Features = 8, Num Classes = 2\n",
      "\n",
      "EMCI-AD-random Dataset\n",
      "Data(x=[35, 8], edge_index=[2, 1178], edge_attr=[1178], y=[1])\n",
      "Num Features = 8, Num Classes = 2\n",
      "\n",
      "EMCI-AD-laplacian Dataset\n",
      "Data(x=[35, 8], edge_index=[2, 1178], edge_attr=[1178], y=[1])\n",
      "Num Features = 8, Num Classes = 2\n",
      "\n",
      "SLIM160-ones Dataset\n",
      "Data(x=[160, 8], edge_index=[2, 19994], edge_attr=[19994], y=[1])\n",
      "Num Features = 8, Num Classes = 3\n",
      "\n",
      "SLIM160-random Dataset\n",
      "Data(x=[160, 8], edge_index=[2, 19994], edge_attr=[19994], y=[1])\n",
      "Num Features = 8, Num Classes = 3\n",
      "\n",
      "SLIM160-laplacian Dataset\n",
      "Data(x=[160, 8], edge_index=[2, 19994], edge_attr=[19994], y=[1])\n",
      "Num Features = 8, Num Classes = 3\n"
     ]
    }
   ],
   "source": [
    "## Simulated dataset\n",
    "simulated_ones, simulated_ones_num_features, simulated_ones_num_classes = process_connectomic_dataset(\n",
    "    'Simulated-ones', simulated_adj, simulated_ones_features)\n",
    "simulated_random, simulated_random_num_features, simulated_random_num_classes = process_connectomic_dataset(\n",
    "    'Simulated-random', simulated_adj, simulated_random_features)\n",
    "simulated_laplacian, simulated_laplacian_num_features, simulated_laplacian_num_classes = process_connectomic_dataset(\n",
    "    'Simulated-laplacian', simulated_adj, simulated_laplacian_features)\n",
    "\n",
    "## EMCI-AD dataset\n",
    "emci_ones, emci_ones_num_features, emci_ones_num_classes = process_connectomic_dataset(\n",
    "    'EMCI-AD-ones', emci_adj, emci_ones_features)\n",
    "emci_random, emci_random_num_features, emci_random_num_classes = process_connectomic_dataset(\n",
    "    'EMCI-AD-random', emci_adj, emci_random_features)\n",
    "emci_laplacian, emci_laplacian_num_features, emci_laplacian_num_classes = process_connectomic_dataset(\n",
    "    'EMCI-AD-laplacian', emci_adj, emci_laplacian_features)\n",
    "\n",
    "## slim160 dataset\n",
    "slim160_ones, slim160_ones_num_features, slim160_ones_num_classes = process_connectomic_dataset(\n",
    "    'SLIM160-ones', slim160_adj, slim160_ones_features, batch_size=1)\n",
    "slim160_random, slim160_random_num_features, slim160_random_num_classes = process_connectomic_dataset(\n",
    "    'SLIM160-random', slim160_adj, slim160_random_features)\n",
    "slim160_laplacian, slim160_laplacian_num_features, slim160_laplacian_num_classes = process_connectomic_dataset(\n",
    "    'SLIM160-laplacian', slim160_adj, slim160_laplacian_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## GCN Models\n",
    "class GCN(nn.Module):\n",
    "    def __init__(self, in_features, out_features, bias=True):\n",
    "        super(GCN, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.weight = nn.Parameter(torch.FloatTensor(in_features, out_features))\n",
    "        if bias:\n",
    "            self.bias = nn.Parameter(torch.FloatTensor(out_features))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "        self.reset_parameters() \n",
    "\n",
    "    def reset_parameters(self):\n",
    "        stdv = 1. / math.sqrt(self.weight.size(1))\n",
    "        self.weight.data.uniform_(-stdv, stdv)\n",
    "        if self.bias is not None:\n",
    "            self.bias.data.uniform_(-stdv, stdv)\n",
    "\n",
    "    def forward(self, input, adj):\n",
    "        support = torch.mm(input, self.weight)\n",
    "        output = torch.spmm(adj, support)\n",
    "        if self.bias is not None:\n",
    "            return output + self.bias\n",
    "        else:\n",
    "            return output\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + ' (' \\\n",
    "               + str(self.in_features) + ' -> ' \\\n",
    "               + str(self.out_features) + ')'\n",
    "\n",
    "class GCN1Layer(torch.nn.Module):\n",
    "    def __init__(self, num_features, hidden_features, num_classes):\n",
    "        super(GCN1Layer, self).__init__()\n",
    "        self.gcn1 = GCN(num_features, hidden_features)\n",
    "        self.bn1 = nn.BatchNorm1d(hidden_features)\n",
    "\n",
    "        self.fc = torch.nn.Linear(hidden_features, num_classes)\n",
    "\n",
    "    def forward(self, x, adj, batch):\n",
    "        x = F.relu(self.bn1(self.gcn1(x, adj)))\n",
    "        x = global_mean_pool(x, batch)  \n",
    "        x = self.fc(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "    \n",
    "    def count_parameters(self):\n",
    "        return sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
    "\n",
    "class GCN2Layer(torch.nn.Module):\n",
    "    def __init__(self, num_features, hidden_features, num_classes):\n",
    "        super(GCN2Layer, self).__init__()\n",
    "        self.gcn1 = GCN(num_features, hidden_features)\n",
    "        self.bn1 = nn.BatchNorm1d(hidden_features)\n",
    "        self.gcn2 = GCN(hidden_features, hidden_features*2)\n",
    "        self.bn2 = nn.BatchNorm1d(hidden_features*2)\n",
    "\n",
    "        self.fc = torch.nn.Linear(hidden_features*2, num_classes)\n",
    "\n",
    "    def forward(self, x, adj, batch):\n",
    "        x = F.relu(self.bn1(self.gcn1(x, adj)))\n",
    "        x = F.relu(self.bn2(self.gcn2(x, adj)))\n",
    "        x = global_mean_pool(x, batch)  # Global mean pooling\n",
    "        x = self.fc(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "    \n",
    "    def count_parameters(self):\n",
    "        return sum(p.numel() for p in self.parameters() if p.requires_grad) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trainings & Evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Single Functions\n",
    "def single_train(model, loader, val_loader, lr=0.001, num_epochs=100, patience=5, \n",
    "                step_size=50, gamma=0.5, save_path='models/gcn_x.pth', \n",
    "                binary_classification=True, is_esn=False):\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = torch.nn.NLLLoss()\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=gamma)\n",
    "    if is_esn == True:\n",
    "        spectral_hook = SpectralRadiusOptimizerHook(model)\n",
    "\n",
    "    training_loss = []\n",
    "    validation_loss = []\n",
    "    epoch_time = []\n",
    "    cpu_usage_percent = []\n",
    "    memory_usage = []\n",
    "    gpu_usage = []\n",
    "    gpu_usage_percent = []\n",
    "\n",
    "    best_train_loss = float('inf')\n",
    "    best_val_loss = float('inf')\n",
    "    epochs_no_improve = 0\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    # Get the process object for the current process\n",
    "    process = psutil.Process()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_loss = 0\n",
    "\n",
    "        set_seed(42)\n",
    "        model.train()\n",
    "        epoch_start_time = time.time()\n",
    "\n",
    "        # Measure CPU and GPU usage before the epoch\n",
    "        cpu_usage_before = psutil.cpu_percent(interval=None)\n",
    "        memory_before = process.memory_info().rss\n",
    "        if torch.cuda.is_available():\n",
    "            gpu_usage_before = torch.cuda.memory_allocated(device)\n",
    "            gpu_util_before = torch.cuda.utilization(device)\n",
    "        else:\n",
    "            gpu_usage_before = 0\n",
    "            gpu_util_before = 0\n",
    "\n",
    "        for data in loader:\n",
    "            x, edge_index, batch, y = data.x.to(device), data.edge_index.to(device), data.batch.to(device), data.y.to(device)\n",
    "            adj_matrix = utils.to_dense_adj(edge_index).squeeze(0).to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output = model(x, adj_matrix, batch)\n",
    "            loss = criterion(output, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if is_esn==True:\n",
    "                spectral_hook()\n",
    "\n",
    "            epoch_loss += loss.item() * data.num_graphs\n",
    "        \n",
    "        scheduler.step()  # Step the learning rate scheduler\n",
    "\n",
    "        epoch_end_time = time.time()\n",
    "        epoch_time.append(epoch_end_time - epoch_start_time)\n",
    "        \n",
    "        # Measure CPU and GPU usage after the epoch\n",
    "        cpu_usage_after = psutil.cpu_percent(interval=None)\n",
    "        memory_after = process.memory_info().rss\n",
    "        if torch.cuda.is_available():\n",
    "            gpu_usage_after = torch.cuda.memory_allocated(device)\n",
    "            gpu_util_after = torch.cuda.utilization(device)\n",
    "        else:\n",
    "            gpu_usage_after = 0\n",
    "            gpu_util_after = 0\n",
    "\n",
    "        # Calculate average CPU and GPU usage during the epoch\n",
    "        cpu_usage_percent.append((cpu_usage_before + cpu_usage_after) / 2)\n",
    "        memory_usage.append((memory_before + memory_after) / 2 / (1024**3))  # Convert to GB\n",
    "        gpu_usage.append((gpu_usage_before + gpu_usage_after) / 2 / (1024**3))  # Convert to GB\n",
    "        gpu_usage_percent.append((gpu_util_before + gpu_util_after) / 2)\n",
    "\n",
    "        training_loss.append(epoch_loss)\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for val_data in val_loader:\n",
    "                x, edge_index, batch, y = val_data.x.to(device), val_data.edge_index.to(device), val_data.batch.to(device), val_data.y.to(device)\n",
    "                adj_matrix = utils.to_dense_adj(edge_index).squeeze(0).to(device)\n",
    "                val_output = model(x, adj_matrix, batch)\n",
    "                val_loss += criterion(val_output, y).item() * val_data.num_graphs\n",
    "\n",
    "        validation_loss.append(val_loss)\n",
    "\n",
    "        # Early stopping logic considering both training and validation loss\n",
    "        if val_loss < best_val_loss or epoch_loss < best_train_loss:\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "            if epoch_loss < best_train_loss:\n",
    "                best_train_loss = epoch_loss\n",
    "            epochs_no_improve = 0\n",
    "            total_epoch = epoch + 1\n",
    "            torch.save(model.state_dict(), save_path)  # Save the best model\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            if epochs_no_improve >= patience:\n",
    "                total_epoch = epoch + 1\n",
    "                break\n",
    "\n",
    "    avg_epoch_time = np.mean(epoch_time)\n",
    "    avg_cpu_usage_percent = np.mean(cpu_usage_percent)\n",
    "    avg_memory_usage = np.mean(memory_usage)\n",
    "    avg_gpu_usage = np.mean(gpu_usage)\n",
    "    avg_gpu_usage_percent = np.mean(gpu_usage_percent)\n",
    "    total_training_time = np.sum(epoch_time)\n",
    "    max_cpu_usage_percent = np.max(cpu_usage_percent)\n",
    "    max_memory_usage = np.max(memory_usage)\n",
    "    max_gpu_usage = np.max(gpu_usage)\n",
    "    max_gpu_usage_percent = np.max(gpu_usage_percent)\n",
    "\n",
    "    return total_epoch, total_training_time, avg_memory_usage, avg_gpu_usage, max_memory_usage, max_gpu_usage\n",
    "\n",
    "def binary_evaluation(y_true, y_pred):\n",
    "    # Convert to numpy arrays for easier manipulation\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    \n",
    "    # Calculate True Positives, True Negatives, False Positives, False Negatives\n",
    "    TP = np.sum((y_true == 1) & (y_pred == 1))\n",
    "    TN = np.sum((y_true == 0) & (y_pred == 0))\n",
    "    FP = np.sum((y_true == 0) & (y_pred == 1))\n",
    "    FN = np.sum((y_true == 1) & (y_pred == 0))\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
    "    sensitivity = TP / (TP + FN) if (TP + FN) != 0 else 0\n",
    "    specificity = TN / (TN + FP) if (TN + FP) != 0 else 0\n",
    "    \n",
    "    return accuracy, sensitivity, specificity\n",
    "\n",
    "def multiclass_evaluation(y_true, y_pred):\n",
    "    # Convert to numpy arrays for easier manipulation\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "\n",
    "    # Calculate confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "    # Calculate metrics\n",
    "    accuracy = np.trace(cm) / np.sum(cm)\n",
    "    sensitivity = np.zeros(cm.shape[0])\n",
    "    specificity = np.zeros(cm.shape[0])\n",
    "    \n",
    "    for i in range(cm.shape[0]):\n",
    "        TP = cm[i, i]\n",
    "        FN = np.sum(cm[i, :]) - TP\n",
    "        FP = np.sum(cm[:, i]) - TP\n",
    "        TN = np.sum(cm) - (TP + FN + FP)\n",
    "\n",
    "        sensitivity[i] = TP / (TP + FN) if (TP + FN) != 0 else 0\n",
    "        specificity[i] = TN / (TN + FP) if (TN + FP) != 0 else 0\n",
    "\n",
    "    avg_sensitivity = np.mean(sensitivity)\n",
    "    avg_specificity = np.mean(specificity)\n",
    "\n",
    "    return accuracy, avg_sensitivity, avg_specificity\n",
    "\n",
    "def inference_performance(model, loader):\n",
    "    model.eval()\n",
    "    total_inference_time = 0\n",
    "    cpu_usage_percent = []\n",
    "    memory_usage = []\n",
    "    gpu_usage = []\n",
    "    gpu_usage_percent = []\n",
    "    labels = []\n",
    "\n",
    "    # Get the process object for the current process\n",
    "    process = psutil.Process()\n",
    "\n",
    "    for data in loader:\n",
    "        x, edge_index, batch = data.x.to(device), data.edge_index.to(device), data.batch.to(device)\n",
    "        adj_matrix = utils.to_dense_adj(edge_index).squeeze(0).to(device)\n",
    "        \n",
    "        # Measure CPU and GPU usage before inference\n",
    "        cpu_usage_before = psutil.cpu_percent(interval=None)\n",
    "        memory_before = process.memory_info().rss\n",
    "        if torch.cuda.is_available():\n",
    "            gpu_usage_before = torch.cuda.memory_allocated(device)\n",
    "            gpu_util_before = torch.cuda.utilization(device)\n",
    "        else:\n",
    "            gpu_usage_before = 0\n",
    "            gpu_util_before = 0\n",
    "\n",
    "        start_time = time.time()\n",
    "        with torch.no_grad():\n",
    "            output = model(x, adj_matrix, batch)\n",
    "            labels.append(data.y.cpu().numpy())\n",
    "        end_time = time.time()\n",
    "\n",
    "        # Measure CPU and GPU usage after inference\n",
    "        cpu_usage_after = psutil.cpu_percent(interval=None)\n",
    "        memory_after = process.memory_info().rss\n",
    "        if torch.cuda.is_available():\n",
    "            gpu_usage_after = torch.cuda.memory_allocated(device)\n",
    "            gpu_util_after = torch.cuda.utilization(device)\n",
    "        else:\n",
    "            gpu_usage_after = 0\n",
    "            gpu_util_after = 0\n",
    "\n",
    "        # Calculate average CPU and GPU usage during inference\n",
    "        cpu_usage_percent.append((cpu_usage_before + cpu_usage_after) / 2)\n",
    "        memory_usage.append((memory_before + memory_after) / 2 / (1024**3))  # Convert to GB\n",
    "        gpu_usage.append((gpu_usage_before + gpu_usage_after) / 2 / (1024**3))  # Convert to GB\n",
    "        gpu_usage_percent.append((gpu_util_before + gpu_util_after) / 2)\n",
    "\n",
    "        # Calculate inference time\n",
    "        inference_time = end_time - start_time\n",
    "        total_inference_time += inference_time\n",
    "\n",
    "    avg_cpu_usage_percent = np.mean(cpu_usage_percent)\n",
    "    avg_memory_usage = np.mean(memory_usage)\n",
    "    avg_gpu_usage = np.mean(gpu_usage)\n",
    "    avg_gpu_usage_percent = np.mean(gpu_usage_percent)\n",
    "    total_inference_time = total_inference_time / len(loader)  # Average inference time per batch\n",
    "\n",
    "    print(f'\\nAverage Inference Time per Batch: {total_inference_time:.4f}s')\n",
    "    print(f'Average CPU Usage: {avg_cpu_usage_percent:.2f}%')\n",
    "    print(f'Average Memory Usage: {avg_memory_usage:.2f}GB')\n",
    "    print(f'Average GPU Usage: {avg_gpu_usage:.2f}GB')\n",
    "    print(f'Average GPU Utilization: {avg_gpu_usage_percent:.2f}%')\n",
    "\n",
    "    return \n",
    "\n",
    "## Cross Validation\n",
    "def single_train_test_cv(model, dataset, num_folds=3, lr=0.001, num_epochs=100, patience=10, step_size=50, gamma=0.5, \n",
    "                         save_path='models/gcn_x_cv.pth', binary_classification=True, is_esn=False):\n",
    "\n",
    "    kfold = KFold(n_splits=num_folds, shuffle=True, random_state=42)\n",
    "\n",
    "    fold_results = {\n",
    "        \"accuracy\": [],\n",
    "        \"sensitivity\": [],\n",
    "        \"specificity\": [],\n",
    "        \"epochs\": [],\n",
    "        \"training_times\": [],\n",
    "        \"testing_times\": [],\n",
    "        \"avg_memory_usage\": [],\n",
    "        \"avg_gpu_usage\": [],\n",
    "        \"max_memory_usage\": [],\n",
    "        \"max_gpu_usage\": []\n",
    "    }\n",
    "\n",
    "    for fold, (train_val_idx, test_idx) in enumerate(kfold.split(dataset)):\n",
    "        print(f\"\\nFold {fold+1}/{num_folds}\")\n",
    "        \n",
    "        set_seed(fold)\n",
    "        # Create samplers\n",
    "        train_val_sampler = SubsetRandomSampler(train_val_idx)\n",
    "        test_sampler = SubsetRandomSampler(test_idx)\n",
    "        \n",
    "        # Create data loaders with deterministic behavior\n",
    "        train_val_loader = DataLoader(dataset, sampler=train_val_sampler, batch_size=32, num_workers=0)\n",
    "        test_loader = DataLoader(dataset, sampler=test_sampler, batch_size=32, num_workers=0)\n",
    "        \n",
    "        # Split train_val indices into train and validation indices\n",
    "        train_size = int(0.9 * len(train_val_idx))\n",
    "        val_size = len(train_val_idx) - train_size\n",
    "        train_indices, val_indices = train_val_idx[:train_size], train_val_idx[train_size:]\n",
    "\n",
    "        # Create subsets\n",
    "        train_dataset = Subset(dataset, train_indices)\n",
    "        val_dataset = Subset(dataset, val_indices)\n",
    "        \n",
    "        # Create data loaders\n",
    "        train_loader = DataLoader(train_dataset, batch_size=32, worker_init_fn=seed_worker, num_workers=0)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=32, worker_init_fn=seed_worker, num_workers=0)\n",
    "        \n",
    "        # Initialize model\n",
    "        model_copy = copy.deepcopy(model)\n",
    "        \n",
    "        # Train the model\n",
    "        # set_seed(42)\n",
    "        total_epoch, total_training_time, avg_memory_usage, avg_gpu_usage, max_memory_usage, max_gpu_usage = single_train(\n",
    "            model_copy, train_loader, val_loader, lr=lr, num_epochs=num_epochs, patience=patience, \n",
    "            step_size=step_size, gamma=gamma, save_path=save_path, \n",
    "            binary_classification=binary_classification, is_esn=is_esn)\n",
    "        \n",
    "        # Test the model\n",
    "        # set_seed(42)\n",
    "        accuracy, sensitivity, specificity, testing_time = multi_test(model_copy.to(device), test_loader, binary_classification=binary_classification)\n",
    "\n",
    "        # Collect metrics for the fold\n",
    "        fold_results[\"accuracy\"].append(accuracy)\n",
    "        fold_results[\"sensitivity\"].append(sensitivity)\n",
    "        fold_results[\"specificity\"].append(specificity)\n",
    "        fold_results[\"epochs\"].append(total_epoch)\n",
    "        fold_results[\"training_times\"].append(total_training_time)\n",
    "        fold_results[\"testing_times\"].append(testing_time)\n",
    "        fold_results[\"avg_memory_usage\"].append(avg_memory_usage)\n",
    "        fold_results[\"avg_gpu_usage\"].append(avg_gpu_usage)\n",
    "        fold_results[\"max_memory_usage\"].append(max_memory_usage)\n",
    "        fold_results[\"max_gpu_usage\"].append(max_gpu_usage)\n",
    "\n",
    "        print(f\"Fold {fold+1} Results:\")\n",
    "        print(f\"   Accuracy: {accuracy:.4f}\")\n",
    "        print(f\"   Average Sensitivity (Recall): {sensitivity:.4f}\")\n",
    "        print(f\"   Average Specificity: {specificity:.4f}\")\n",
    "\n",
    "    # Compute average metrics across all folds\n",
    "    avg_metrics = {metric: np.mean(fold_results[metric]) for metric in fold_results}\n",
    "    std_metrics = {metric: np.std(fold_results[metric]) for metric in fold_results}\n",
    "\n",
    "    print(\"\\nCross-Validation Results:\")\n",
    "    for metric in avg_metrics:\n",
    "        avg_value = avg_metrics[metric]\n",
    "        std_value = std_metrics[metric]\n",
    "        print(f\"{metric}: Mean = {avg_value:.4f} ± {std_value:.2f}\")\n",
    "\n",
    "    return avg_metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulated Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GCN2Layer(\n",
      "  (gcn1): GCN (8 -> 16)\n",
      "  (bn1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (gcn2): GCN (16 -> 32)\n",
      "  (bn2): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc): Linear(in_features=32, out_features=3, bias=True)\n",
      ")\n",
      "Total number of trainable parameters: 883\n",
      "\n",
      "\n",
      "Fold 1/3\n",
      "Fold 1 Results:\n",
      "   Accuracy: 0.2900\n",
      "   Average Sensitivity (Recall): 0.3333\n",
      "   Average Specificity: 0.6667\n",
      "\n",
      "Fold 2/3\n",
      "Fold 2 Results:\n",
      "   Accuracy: 0.3600\n",
      "   Average Sensitivity (Recall): 0.3411\n",
      "   Average Specificity: 0.6703\n",
      "\n",
      "Fold 3/3\n",
      "Fold 3 Results:\n",
      "   Accuracy: 0.4400\n",
      "   Average Sensitivity (Recall): 0.4353\n",
      "   Average Specificity: 0.7177\n",
      "\n",
      "Cross-Validation Results:\n",
      "accuracy: Mean = 0.3633 ± 0.06\n",
      "sensitivity: Mean = 0.3699 ± 0.05\n",
      "specificity: Mean = 0.6849 ± 0.02\n",
      "epochs: Mean = 325.0000 ± 70.60\n",
      "training_times: Mean = 9.5425 ± 2.05\n",
      "testing_times: Mean = 0.0111 ± 0.00\n",
      "avg_memory_usage: Mean = 0.1657 ± 0.01\n",
      "avg_gpu_usage: Mean = 0.0000 ± 0.00\n",
      "max_memory_usage: Mean = 0.1715 ± 0.01\n",
      "max_gpu_usage: Mean = 0.0000 ± 0.00\n"
     ]
    }
   ],
   "source": [
    "set_seed(42)\n",
    "gcn_simulated_ones = GCN2Layer(simulated_ones_num_features, 2*simulated_ones_num_features, simulated_ones_num_classes).to(device)\n",
    "print(gcn_simulated_ones)\n",
    "print(f\"Total number of trainable parameters: {(gcn_simulated_ones.count_parameters())}\\n\")\n",
    "avg_metrics = single_train_test_cv(gcn_simulated_ones, simulated_ones, \n",
    "            lr=0.005, num_epochs=500, step_size=500,  \n",
    "            save_path='models/gcn_simulated_ones.pth', binary_classification=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GCN2Layer(\n",
      "  (gcn1): GCN (8 -> 16)\n",
      "  (bn1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (gcn2): GCN (16 -> 32)\n",
      "  (bn2): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc): Linear(in_features=32, out_features=3, bias=True)\n",
      ")\n",
      "Total number of trainable parameters: 883\n",
      "\n",
      "\n",
      "Fold 1/3\n",
      "Fold 1 Results:\n",
      "   Accuracy: 0.3400\n",
      "   Average Sensitivity (Recall): 0.3599\n",
      "   Average Specificity: 0.6799\n",
      "\n",
      "Fold 2/3\n",
      "Fold 2 Results:\n",
      "   Accuracy: 0.4000\n",
      "   Average Sensitivity (Recall): 0.4269\n",
      "   Average Specificity: 0.7102\n",
      "\n",
      "Fold 3/3\n",
      "Fold 3 Results:\n",
      "   Accuracy: 0.3700\n",
      "   Average Sensitivity (Recall): 0.3671\n",
      "   Average Specificity: 0.6857\n",
      "\n",
      "Cross-Validation Results:\n",
      "accuracy: Mean = 0.3700 ± 0.02\n",
      "sensitivity: Mean = 0.3846 ± 0.03\n",
      "specificity: Mean = 0.6919 ± 0.01\n",
      "epochs: Mean = 217.3333 ± 199.92\n",
      "training_times: Mean = 6.4085 ± 5.79\n",
      "testing_times: Mean = 0.0115 ± 0.00\n",
      "avg_memory_usage: Mean = 0.1430 ± 0.01\n",
      "avg_gpu_usage: Mean = 0.0000 ± 0.00\n",
      "max_memory_usage: Mean = 0.1475 ± 0.01\n",
      "max_gpu_usage: Mean = 0.0000 ± 0.00\n"
     ]
    }
   ],
   "source": [
    "set_seed(42)\n",
    "gcn_simulated_random = GCN2Layer(simulated_random_num_features, 2*simulated_random_num_features, simulated_random_num_classes).to(device)\n",
    "print(gcn_simulated_random)\n",
    "print(f\"Total number of trainable parameters: {(gcn_simulated_random.count_parameters())}\\n\")\n",
    "avg_metrics = single_train_test_cv(gcn_simulated_random, simulated_random, \n",
    "            lr=0.005, num_epochs=500, step_size=500,  \n",
    "            save_path='models/gcn_simulated_random.pth', binary_classification=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GCN2Layer(\n",
      "  (gcn1): GCN (8 -> 16)\n",
      "  (bn1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (gcn2): GCN (16 -> 32)\n",
      "  (bn2): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc): Linear(in_features=32, out_features=3, bias=True)\n",
      ")\n",
      "Total number of trainable parameters: 883\n",
      "\n",
      "\n",
      "Fold 1/3\n",
      "Fold 1 Results:\n",
      "   Accuracy: 0.3600\n",
      "   Average Sensitivity (Recall): 0.3425\n",
      "   Average Specificity: 0.6677\n",
      "\n",
      "Fold 2/3\n",
      "Fold 2 Results:\n",
      "   Accuracy: 0.3400\n",
      "   Average Sensitivity (Recall): 0.3512\n",
      "   Average Specificity: 0.6740\n",
      "\n",
      "Fold 3/3\n",
      "Fold 3 Results:\n",
      "   Accuracy: 0.3800\n",
      "   Average Sensitivity (Recall): 0.3683\n",
      "   Average Specificity: 0.6842\n",
      "\n",
      "Cross-Validation Results:\n",
      "accuracy: Mean = 0.3600 ± 0.02\n",
      "sensitivity: Mean = 0.3540 ± 0.01\n",
      "specificity: Mean = 0.6753 ± 0.01\n",
      "epochs: Mean = 223.0000 ± 195.94\n",
      "training_times: Mean = 6.4610 ± 5.66\n",
      "testing_times: Mean = 0.0100 ± 0.00\n",
      "avg_memory_usage: Mean = 0.1524 ± 0.01\n",
      "avg_gpu_usage: Mean = 0.0000 ± 0.00\n",
      "max_memory_usage: Mean = 0.1594 ± 0.01\n",
      "max_gpu_usage: Mean = 0.0000 ± 0.00\n"
     ]
    }
   ],
   "source": [
    "set_seed(42)\n",
    "gcn_simulated_laplacian = GCN2Layer(simulated_laplacian_num_features, 2*simulated_laplacian_num_features, simulated_laplacian_num_classes).to(device)\n",
    "print(gcn_simulated_laplacian)\n",
    "print(f\"Total number of trainable parameters: {(gcn_simulated_laplacian.count_parameters())}\\n\")\n",
    "avg_metrics = single_train_test_cv(gcn_simulated_laplacian, simulated_laplacian, \n",
    "            lr=0.005, num_epochs=500, step_size=500,  \n",
    "            save_path='models/gcn_simulated_laplacian.pth', binary_classification=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EMCI-AD Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GCN2Layer(\n",
      "  (gcn1): GCN (8 -> 16)\n",
      "  (bn1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (gcn2): GCN (16 -> 32)\n",
      "  (bn2): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc): Linear(in_features=32, out_features=2, bias=True)\n",
      ")\n",
      "Total number of trainable parameters: 850\n",
      "\n",
      "\n",
      "Fold 1/3\n",
      "Fold 1 Results:\n",
      "   Accuracy: 0.5111\n",
      "   Average Sensitivity (Recall): 0.0000\n",
      "   Average Specificity: 1.0000\n",
      "\n",
      "Fold 2/3\n",
      "Fold 2 Results:\n",
      "   Accuracy: 0.4444\n",
      "   Average Sensitivity (Recall): 1.0000\n",
      "   Average Specificity: 0.0000\n",
      "\n",
      "Fold 3/3\n",
      "Fold 3 Results:\n",
      "   Accuracy: 0.4318\n",
      "   Average Sensitivity (Recall): 0.0000\n",
      "   Average Specificity: 1.0000\n",
      "\n",
      "Cross-Validation Results:\n",
      "accuracy: Mean = 0.4625 ± 0.03\n",
      "sensitivity: Mean = 0.3333 ± 0.47\n",
      "specificity: Mean = 0.6667 ± 0.47\n",
      "epochs: Mean = 24.6667 ± 7.32\n",
      "training_times: Mean = 0.3475 ± 0.10\n",
      "testing_times: Mean = 0.0050 ± 0.00\n",
      "avg_memory_usage: Mean = 0.1657 ± 0.00\n",
      "avg_gpu_usage: Mean = 0.0000 ± 0.00\n",
      "max_memory_usage: Mean = 0.1671 ± 0.00\n",
      "max_gpu_usage: Mean = 0.0000 ± 0.00\n"
     ]
    }
   ],
   "source": [
    "set_seed(42)\n",
    "gcn_emci_ones = GCN2Layer(emci_ones_num_features, 2*emci_ones_num_features, emci_ones_num_classes).to(device)\n",
    "print(gcn_emci_ones)\n",
    "print(f\"Total number of trainable parameters: {(gcn_emci_ones.count_parameters())}\\n\")\n",
    "avg_metrics = single_train_test_cv(gcn_emci_ones, emci_ones, \n",
    "            lr=0.005, num_epochs=500, step_size=500,  \n",
    "            save_path='models/gcn_emci_ones.pth', binary_classification=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GCN2Layer(\n",
      "  (gcn1): GCN (8 -> 16)\n",
      "  (bn1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (gcn2): GCN (16 -> 32)\n",
      "  (bn2): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc): Linear(in_features=32, out_features=2, bias=True)\n",
      ")\n",
      "Total number of trainable parameters: 850\n",
      "\n",
      "\n",
      "Fold 1/3\n",
      "Fold 1 Results:\n",
      "   Accuracy: 0.5556\n",
      "   Average Sensitivity (Recall): 0.5455\n",
      "   Average Specificity: 0.5652\n",
      "\n",
      "Fold 2/3\n",
      "Fold 2 Results:\n",
      "   Accuracy: 0.5778\n",
      "   Average Sensitivity (Recall): 0.8500\n",
      "   Average Specificity: 0.3600\n",
      "\n",
      "Fold 3/3\n",
      "Fold 3 Results:\n",
      "   Accuracy: 0.6364\n",
      "   Average Sensitivity (Recall): 0.4800\n",
      "   Average Specificity: 0.8421\n",
      "\n",
      "Cross-Validation Results:\n",
      "accuracy: Mean = 0.5899 ± 0.03\n",
      "sensitivity: Mean = 0.6252 ± 0.16\n",
      "specificity: Mean = 0.5891 ± 0.20\n",
      "epochs: Mean = 500.0000 ± 0.00\n",
      "training_times: Mean = 6.7980 ± 0.19\n",
      "testing_times: Mean = 0.0046 ± 0.00\n",
      "avg_memory_usage: Mean = 0.1648 ± 0.00\n",
      "avg_gpu_usage: Mean = 0.0000 ± 0.00\n",
      "max_memory_usage: Mean = 0.1724 ± 0.00\n",
      "max_gpu_usage: Mean = 0.0000 ± 0.00\n"
     ]
    }
   ],
   "source": [
    "set_seed(42)\n",
    "gcn_emci_random = GCN2Layer(emci_random_num_features, 2*emci_random_num_features, emci_random_num_classes).to(device)\n",
    "print(gcn_emci_random)\n",
    "print(f\"Total number of trainable parameters: {(gcn_emci_random.count_parameters())}\\n\")\n",
    "avg_metrics = single_train_test_cv(gcn_emci_random, emci_random, \n",
    "            lr=0.005, num_epochs=500, step_size=500,  \n",
    "            save_path='models/gcn_emci_random.pth', binary_classification=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GCN2Layer(\n",
      "  (gcn1): GCN (8 -> 16)\n",
      "  (bn1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (gcn2): GCN (16 -> 32)\n",
      "  (bn2): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc): Linear(in_features=32, out_features=2, bias=True)\n",
      ")\n",
      "Total number of trainable parameters: 850\n",
      "\n",
      "\n",
      "Fold 1/3\n",
      "Fold 1 Results:\n",
      "   Accuracy: 0.4889\n",
      "   Average Sensitivity (Recall): 1.0000\n",
      "   Average Specificity: 0.0000\n",
      "\n",
      "Fold 2/3\n",
      "Fold 2 Results:\n",
      "   Accuracy: 0.4444\n",
      "   Average Sensitivity (Recall): 1.0000\n",
      "   Average Specificity: 0.0000\n",
      "\n",
      "Fold 3/3\n",
      "Fold 3 Results:\n",
      "   Accuracy: 0.5682\n",
      "   Average Sensitivity (Recall): 1.0000\n",
      "   Average Specificity: 0.0000\n",
      "\n",
      "Cross-Validation Results:\n",
      "accuracy: Mean = 0.5005 ± 0.05\n",
      "sensitivity: Mean = 1.0000 ± 0.00\n",
      "specificity: Mean = 0.0000 ± 0.00\n",
      "epochs: Mean = 72.3333 ± 5.31\n",
      "training_times: Mean = 1.0024 ± 0.05\n",
      "testing_times: Mean = 0.0046 ± 0.00\n",
      "avg_memory_usage: Mean = 0.3332 ± 0.01\n",
      "avg_gpu_usage: Mean = 0.0000 ± 0.00\n",
      "max_memory_usage: Mean = 0.3448 ± 0.03\n",
      "max_gpu_usage: Mean = 0.0000 ± 0.00\n"
     ]
    }
   ],
   "source": [
    "set_seed(42)\n",
    "gcn_emci_laplacian = GCN2Layer(emci_laplacian_num_features, 2*emci_laplacian_num_features, emci_laplacian_num_classes).to(device)\n",
    "print(gcn_emci_laplacian)\n",
    "print(f\"Total number of trainable parameters: {(gcn_emci_laplacian.count_parameters())}\\n\")\n",
    "avg_metrics = single_train_test_cv(gcn_emci_laplacian, emci_laplacian, \n",
    "            lr=0.005, num_epochs=500, step_size=500,  \n",
    "            save_path='models/gcn_emci_laplacian.pth', binary_classification=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
